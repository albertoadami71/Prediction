---
title: 'Using machine learning to predict the quality of the exercise'
output: html_document
---
## Summary

In this project, we worked with the data obtained by reading accelerometers installed in the body of 6 subjects, while practicing physical exercises. The devices were placed on the belt, forearm, arm and dumbbell, each of which produced acceleration readings in the x, y and z axis.
Each physical activity was carried out in specific conditions so that they can be classified into A, B, C, D or E, according to their quality. More information on the data used can be obtained at http://groupware.les.inf.puc-rio.br/har site.
The data set was divided into two subsets, one for training and one for testing. In the training phase, the training data are used to determine the appropriate parameters for the prediction model. Once the parameters have been adjusted to the training set, we apply the model in the testing set to verify the percentage of correct predictions. This number gives us a measure of the model efficiency (accuracy).
As a starting point, we use the predictive models gbm (Generalized Boosted Models) and rf (Random Forests), which are considered the best predictive models, winners of competitions, as we learned in the course. Nevertheless, the results were not enough to find the correct answers to all 20 test cases that have been proposed in the course (I think I did not use the methods correctly). Therefore, we prepared a homemade method that achieved total success in this case.


## Analysis


Below we prepare the data set, reading data files and separating the training (60%) and testing (40%) subsets. Then we apply the gbm method for determining the prediction model (modelFit). After that, we use the model to make predictions in the testing set and compare the predicted values with the known values, using confusionMatrix.   
```{r, cache=TRUE, message=FALSE, echo=TRUE, results="hide"}
library(caret)
dados<-read.csv("pml-training.csv")
dados1<-read.csv("pml-testing.csv")
extrai<-dados[, ((substr(names(dados),1,6)=="accel_")|(substr(names(dados),1,6)=="classe"))]
inTrain = createDataPartition(dados$classe, p = 0.6)[[1]]
training = extrai[ inTrain,]
testing = extrai[-inTrain,]
 
testfinal<-dados1[, (substr(names(dados1),1,6)=="accel_")]


set.seed(123)
modelFit<-train(training$classe ~., method="gbm", data=training)
```

```{r, cache=TRUE}
predictions <- predict(modelFit, newdata=testing)
print(confusionMatrix(predictions,testing$classe))
```

Applying the model above in the 20 test cases, we find the following result:

```{r, cache=TRUE}
predictions <- predict(modelFit, newdata=testfinal)
print(predictions)

```

Now we repeat the procedure using Random Forests algorithm (rf).
```{r, cache=TRUE, message=FALSE, echo=TRUE, results="hide"}
set.seed(123)
modelFit<-train(training$classe ~., method="rf", data=training)
```

```{r, cache=TRUE}
predictions <- predict(modelFit, newdata=testing)
print(confusionMatrix(predictions,testing$classe))
```
In this case, we get the following answers of the test problems.
```{r, cache=TRUE}
predictions <- predict(modelFit, newdata=testfinal)
print(predictions)
```
Considering that none of the above results has been fully satisfactory in solving test problems, we have designed a very simple home algorithm to predict the exercise classification (A, B, C, D or E) on the basis of the twelve measures reported by accelerometers.
The basic idea is to take a set T of vectors in which the class is known. Mathematically, ti = (ti1, ti2, ti3, ..., ti12) belongs to T and class(ti) = ci, where ci belongs to the set {A, B, C, D, E}.
If we have a vector v = (vi1, vi2, ..., vi12)  for which the class is not known, we calculate the Euclidean distance between v and each of the T elements and then ordered these distances.
Let's assume that tj is the T element that has the shortest distance to the vector v, then we consider that class(v) = class(tj) = cj.
Below, we have the application of the homemade algorithm using set T = training and V = testing. Note that the accuracy for this method is closer to the accuracy of previous methods.
```{r, cache=TRUE, message=FALSE, echo=TRUE}
predaux = data.frame()
tamanho = dim(testing)[1]
for (k in 1:dim(testing)[1]){
#print(k)
#print(tamanho)

za = testing[k,1:12]
tabela = data.frame()
df = za
#
# Suppose we have T = training = rbind(t1, t2, ..., tN), where each ti = (ti1, ti2,...,ti12)
# Then we create V = rbind(v, v, ...,v), where v is repeated N times and v = (v1, v2,...,v12)
# Remember that we know the class ci of each ti (A, B, C, D or E), but we don´t know the class of v
# To calculate the distance between each ti and v, we execute the following R command: 
# dist = rowSums(T-V)^2. Here, it is not necessary to find the square root. 
# Thus, the i component of vector dist is the squared distance between ti and v. 
#
zarep = df[rep(seq_len(nrow(df)), each = dim(training)[1]),]
dist = rowSums((training[,1:12]-zarep)^2 )
tabela = data.frame(indice = (1:dim(training)[1]), distancia = dist, classe = training[,13])
tabela = tabela[order(tabela[,2]),]
#
# We order the dist vector to find the tj that have the minimum (tj-v)^2 value.
# Then we consider that cj is the estimated class to v.
#
predaux[k,1]=k
predaux[k,2]=tabela[1,3]
#print(testing[k,13])
#print(tabela[1,3])
}
confusionMatrix(predaux[,2],testing$classe)

```

From the above results we conclude that, in this case, the performance of homemade method comes close to excellent performance of the rf method.

To obtain the predictions for the classes in the test problems, we use the homemade method considering set T = all available data in Training table. In addition, we consider the first two classes corresponding to the two shorter distances. This second class was used as an additional option. Below are the used code and submitted responses. 
```{r, cache=TRUE, message=FALSE, echo=TRUE}
set.seed(123)
training<-dados[, ((substr(names(dados),1,6)=="accel_")|(substr(names(dados),1,6)=="classe"))]

testfinal<-dados1[, (substr(names(dados1),1,6)=="accel_")]
predaux = data.frame()
tamanho = dim(testfinal)[1]
for (k in 1:dim(testfinal)[1]){
#print(k)
#print(tamanho)

za = testfinal[k,1:12]
tabela = data.frame()
df = za
#
# Suppose we have T = training = rbind(t1, t2, ..., tN), where each ti = (ti1, ti2,...,ti12)
# Then we create V = rbind(v, v, ...,v), where v is repeated N times and v = (v1, v2,...,v12)
# Remember that we know the class ci of each ti (A, B, C, D or E), but we don´t know the class of v
# To calculate the distance between each ti and v, we execute the following R command: 
# dist = rowSums(T-V)^2. Here, it is not necessary to find the square root. 
# Thus, the i component of vector dist is the squared distance between ti and v. 
#
zarep = df[rep(seq_len(nrow(df)), each = dim(training)[1]),]
dist = rowSums((training[,1:12]-zarep)^2 )
tabela = data.frame(indice = (1:dim(training)[1]), distancia = dist, classe = training[,13])
#
# We order the dist vector to find the tj that have the minimum (tj-v)^2 value.
# Then we consider that cj is the estimated class to v.
#
# If tk is the next element in the ordered dist vector, 
# then we consider that ck is a additional option to the v class.

tabela = tabela[order(tabela[,2]),]
predaux[k,1]=k
predaux[k,2]=tabela[1,3]
predaux[k,3]=tabela[2,3]

#print(tabela[1,3])
#print(tabela[2,3])
}
print(predaux[,2:3])

```


## Conclusion

Through this vibrant competition among three algorithms, two famous and one homemade, we had the opportunity to learn the main concepts of machine learning.
